{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform training and testing for the budget of N_QUERIES. The \"unlabeled\" dataset is queried for the most informative points according to our query strategy\n",
    "            # which are consecutively added to the training set after which performance is directly measured.\n",
    "            for index in range(N_QUERIES):\n",
    "                # Setting up kernel with rbf as default and additional parameters (for PAL and xPAL)\n",
    "                params = {}\n",
    "                if AL_METHOD == 6 or AL_METHOD == 7 or AL_METHOD == 8 or AL_METHOD == 9:\n",
    "                    S =  pairwise_kernels(X = X_train, Y = X_train, metric='rbf', gamma=gamma)\n",
    "                    S = check_array(S, np.eye(len(X_train)))\n",
    "                    if SEPARATE_CLASSIFIER:\n",
    "                        params = {'n_classes': len(unique_classes), \"labeled_instances\": query_estimator.X_training, \"labels\": query_estimator.y_training, \"kernel\": S, \"priors\": pal_priors}\n",
    "                    else: \n",
    "                        params = {'n_classes': len(unique_classes), \"labeled_instances\": learner.X_training, \"labels\": learner.y_training, \"kernel\": S, \"priors\": pal_priors}\n",
    "\n",
    "                # Query instance using AL method\n",
    "                if SEPARATE_CLASSIFIER:\n",
    "                    query_index, query_instance = query_estimator.query(X_train, **params)\n",
    "                else:\n",
    "                    query_index, query_instance = learner.query(X_train, **params)\n",
    "\n",
    "                # For measuring frequencies of selection of specific trajectories over the course of learning\n",
    "                trajectory_selection_frequencies.iloc[index][int(X_train[query_index,0])] += 1\n",
    "                \n",
    "                # For measuring performance in terms of label selection\n",
    "                selected_training_instance_labels = np.append(selected_training_instance_labels, y_train[query_index])\n",
    "\n",
    "                # logic to make sure only do additional bootstrapping when using qbc, and only on the QBC classifiers\n",
    "                boot_main = False\n",
    "                boot_sep = False\n",
    "#                 # This currently breaks. The bootstrapping takes random samples, but there are too few in the initialization set to guarantee at least one of each class\n",
    "#                 if AL_METHOD == 5:\n",
    "#                     if SEPARATE_CLASSIFIER:\n",
    "#                         boot_main = False\n",
    "#                         boot_sep = True\n",
    "#                     else:\n",
    "#                         boot_main = True\n",
    "#                         boot_sep = True\n",
    "                \n",
    "                # Setting weighting of training instances when not randomly selecting them. Using two arrays guarantees that no overlapping values erase each other\n",
    "                #if WEIGHTING_FACTOR != 0 and AL_METHOD != 1 and AL_METHOD != 5:\n",
    "                #    current_labels = np.append(learner.y_training, y_train[query_index].reshape(1, ))\n",
    "                #    sample_weights = np.zeros(len(current_labels)).astype(float)\n",
    "                #    temp_indices = np.where(current_labels == NUM_TARGET)\n",
    "                #    sample_weights[temp_indices] = target_weight\n",
    "                #    temp_indices = np.where(current_labels != NUM_TARGET)\n",
    "                #    sample_weights[temp_indices] = non_target_weight\n",
    "                #else:\n",
    "                #    sample_weights = None\n",
    "    \n",
    "        \n",
    "                # Training the consumer with the summarizing queried instance\n",
    "                if TRAIN_ON_SUM:\n",
    "                    # Train our ActiveLearner model with the instance it has requested.\n",
    "                    X_temp, y_temp = X_train[query_index].reshape(1, -1), y_train[query_index].reshape(1, )\n",
    "                    learner.teach(X=X_temp, y=y_temp, bootstrap=boot_main, sample_weight = sample_weights)\n",
    "\n",
    "                    # Training a separate classifier (producer) with the summarizing queried instance\n",
    "                    if SEPARATE_CLASSIFIER:\n",
    "                        X_temp, y_temp = X_train[query_index].reshape(1, -1), y_train[query_index].reshape(1, )\n",
    "                        query_estimator.teach(X=X_temp, y=y_temp, bootstrap=boot_sep, sample_weight = sample_weights)\n",
    "\n",
    "                # Training the model with all instances of queried trip\n",
    "                else:\n",
    "                    # Training a separate classifier (producer) used for selecting queries\n",
    "                    if SEPARATE_CLASSIFIER:\n",
    "                        X_temp, y_temp = X_train[query_index].reshape(1, -1), y_train[query_index].reshape(1, )\n",
    "                        \n",
    "                        #if WEIGHTING_FACTOR != 0 and AL_METHOD != 1 and AL_METHOD != 5:\n",
    "                        #    current_labels = np.append(query_estimator.y_training, y_train[query_index].reshape(1, ))\n",
    "                        #    sample_weights = np.zeros(len(current_labels)).astype(float)\n",
    "                        #    temp_indices = np.where(current_labels == NUM_TARGET)\n",
    "                        #    sample_weights[temp_indices] = target_weight\n",
    "                        #    temp_indices = np.where(current_labels != NUM_TARGET)\n",
    "                        #    sample_weights[temp_indices] = non_target_weight\n",
    "                        #else:\n",
    "                        #    sample_weights = None\n",
    "                        \n",
    "                        query_estimator.teach(X=X_temp, y=y_temp, bootstrap = boot_sep, sample_weight = sample_weights)\n",
    "\n",
    "                    # Find trip_id of selected query in aggregated instances\n",
    "                    query_trajectory_id = X_train[query_index,0]\n",
    "                    # Retrieve all instances with this trajectory id from full data set\n",
    "\n",
    "                    if not isinstance(query_trajectory_id, float):\n",
    "                        query_trajectory_id = query_trajectory_id[0]\n",
    "                    query_trajectory_full = fold_full_df[fold_full_df[trajectory] == query_trajectory_id] # for sep classifier the specific formatting returns one element in two dimensions\n",
    "\n",
    "                    # Split into numpy X and y\n",
    "                    X_train_full = query_trajectory_full.drop(target, axis=1).to_numpy()\n",
    "                    y_train_full = query_trajectory_full[target].to_numpy()\n",
    "                    del(query_trajectory_full)\n",
    "\n",
    "                    #if WEIGHTING_FACTOR != 0 and AL_METHOD != 1 and AL_METHOD != 5:\n",
    "                    #    current_labels = np.append(learner.y_training, y_train_full)\n",
    "                    #    sample_weights = np.zeros(len(current_labels)).astype(float)\n",
    "                    #    temp_indices = np.where(current_labels == NUM_TARGET)\n",
    "                    #    sample_weights[temp_indices] = target_weight\n",
    "                    #    temp_indices = np.where(current_labels != NUM_TARGET)\n",
    "                    #    sample_weights[temp_indices] = 1 - (non_target_weight * 0.1)\n",
    "                    #else:\n",
    "                    #    sample_weights = None\n",
    "                    \n",
    "                    # Add selected instances to training pool and teach the model re-train the model on the new set\n",
    "                    # Train the consumer on either the full or \n",
    "                    learner.teach(X=X_train_full, y=y_train_full, bootstrap=boot_main, sample_weight = sample_weights)\n",
    "\n",
    "                    del(X_train_full)\n",
    "                    del(y_train_full)\n",
    "\n",
    "                # Remove the queried instance from the unlabeled pool.\n",
    "                X_train, y_train = np.delete(X_train, query_index, axis=0), np.delete(y_train, query_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_openml_test(X, Y, k_, execs_, n_queries_, n_init_, train_sum_, sep_class_, ml_method_, ml_sep_method_, al_method_, qbc_learners_, n_qbc_learners_, weighting_factor_, save_figs_, save_ratio_, save_results_, time_point = None, bandwidth_ = 0, pal_priors = 1, sc_pca_ = True):\n",
    "    # Seed to allow different AL methods to train and test with the same splits and initialization sets over the course of the same number of executions with the same settings.\n",
    "    np.random.seed(0)\n",
    "    # Call the set_settings method to change settings as defined by the parameters of this method.\n",
    "    set_settings(k_, execs_, n_queries_, n_init_, train_sum_, sep_class_, ml_method_, ml_sep_method_, al_method_, qbc_learners_, n_qbc_learners_, weighting_factor_, save_figs_, save_ratio_, save_results_, bandwidth_, sc_pca_)\n",
    "    \n",
    "    # Keep track of results for each query. Used later to calculate incremental performance. +1 because the first value is the performance directly after initialization\n",
    "    accuracy_results = pd.DataFrame(columns = range(N_QUERIES + 1))\n",
    "    macro_f1_results = pd.DataFrame(columns = range(N_QUERIES + 1))\n",
    "    recall_results = pd.DataFrame(columns = range(N_QUERIES + 1))\n",
    "    precision_results = pd.DataFrame(columns = range(N_QUERIES + 1))\n",
    "    auc_results = pd.DataFrame(columns = range(N_QUERIES + 1))\n",
    "    \n",
    "     # Perform K-fold validation for EXECUTIONS number of times. This gives more total repetitions, and thus a smoother view of learning performance\n",
    "    for execs in range(EXECUTIONS):\n",
    "        df = X\n",
    "\n",
    "        print(\"Starting execution: \", execs+1, \"/\" , EXECUTIONS,  \"- - - - - - - - \")\n",
    "\n",
    "        unique_classes = Y.unique()\n",
    "        #print(unique_classes)\n",
    "        n_labeled_examples = df.shape[0]\n",
    "\n",
    "        if len(unique_classes) > INITIALIZATION_SET:\n",
    "            print(\"INCREASE SIZE OF INITIALIZATION SET TO INCLUDE AT LEAST ONE INSTANCE OF EACH CLASS\")\n",
    "            raise\n",
    "        \n",
    "        # Make random selection for initialization set based on the sampling ratio and initialization set size\n",
    "        n_minority = math.ceil(int(INITIALIZATION_SET * SAMPLING_RATIO))\n",
    "        n_majority = INITIALIZATION_SET - n_minority\n",
    "        print(n_majority)\n",
    "        #initialization_indices = np.hstack([df[df[target]==NUM_TARGET].sample(n=n_minority, replace=False).index, \\\n",
    "                                                 #df[df[target]!=NUM_TARGET].sample(n=n_majority, replace=False).index])\n",
    "        #initialization_set = df.iloc[initialization_indices]\n",
    "\n",
    "        # Isolate the non-initialization examples to make up the sampling pool.\n",
    "        #df = df.drop(df.index[initialization_indices]).reset_index(drop=True)\n",
    "\n",
    "        # Create separate set of initialization indices if using full trajectories for training\n",
    "        if not TRAIN_ON_SUM:\n",
    "            full_initialization_set = pd.DataFrame(columns = full_df.columns)\n",
    "\n",
    "            for unique_trajectory in initialization_set[trajectory].unique():\n",
    "                # Retrieve all instances with this trajectory id from full data set\n",
    "                full_initialization_set = full_initialization_set.append(full_df[full_df[trajectory] == unique_trajectory])\n",
    "\n",
    "            # Split into X and y\n",
    "            X_initialize = full_initialization_set.drop(target, axis=1).reset_index(drop=True)\n",
    "            y_initialize = full_initialization_set[target].reset_index(drop=True)\n",
    "            del(full_initialization_set)\n",
    "        else:\n",
    "            # Labeled training instances.\n",
    "            X_initialize = initialization_set.drop(target, axis=1).reset_index(drop=True)\n",
    "            y_initialize = initialization_set[target].reset_index(drop=True)\n",
    "            \n",
    "        # labeled training instances.\n",
    "        X_aggr_initialize = initialization_set.drop(target, axis=1).reset_index(drop=True)\n",
    "        y_aggr_initialize = initialization_set[target].reset_index(drop=True)\n",
    "\n",
    "        X_initialize[trajectory] = X_initialize[trajectory].apply(pd.to_numeric)\n",
    "        y_initialize = pd.to_numeric(y_initialize)\n",
    "\n",
    "        # Take stratified folds to make sure each fold contains enough instances of the majority class\n",
    "        skf = StratifiedKFold(n_splits=K, shuffle = True)\n",
    "        \n",
    "        if time_point != None:\n",
    "            pretime_df, ontime_df = time_specific_summarized_set(df, time_point, 30)\n",
    "    \n",
    "            X_aggr = ontime_df.drop(target, axis=1).to_numpy()\n",
    "            y_aggr = ontime_df[target].to_numpy()\n",
    "            train_set_indices = np.array(list(skf.split(X_aggr, y_aggr)))\n",
    "            \n",
    "        else:\n",
    "            # Split aggregated instances into X and y\n",
    "            X_aggr = df.drop(target, axis=1).to_numpy()\n",
    "            y_aggr = df[target].to_numpy()\n",
    "            train_set_indices = np.array(list(skf.split(X_aggr, y_aggr)))\n",
    "            \n",
    "        for i in range(K):\n",
    "            X_train = X_aggr[train_set_indices[i][0]]\n",
    "            y_train = y_aggr[train_set_indices[i][0]]\n",
    "\n",
    "            X_test = X_aggr[train_set_indices[i][1]]\n",
    "            y_test = y_aggr[train_set_indices[i][1]]\n",
    "            \n",
    "            # If the training set has fewer instances than the selected query budget, we have too few instances\n",
    "            if len(X_train) < N_QUERIES:\n",
    "                print(\"TOO FEW TRAINING INSTANCES TO TRAIN THIS FAR. REDUCE TO \", len(X_train))\n",
    "                raise\n",
    "                \n",
    "                \n",
    "            # Save class ratio of instances in training fold\n",
    "            fold_training_ratio = np.count_nonzero(y_train == NUM_TARGET)/np.count_nonzero(y_train != NUM_TARGET) # MOSTLY MAKES SENSE FOR BINARY TARGET CLASS\n",
    "\n",
    "            model = ML_switcher.get(ML_METHOD)\n",
    "            \n",
    "            learner = ActiveLearner(\n",
    "                    estimator=deepcopy(model),\n",
    "                    query_strategy=AL_switcher.get(AL_METHOD),\n",
    "                    X_training=fold_X_initialize.to_numpy(), y_training=y_initialize.to_numpy()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_openml_test(X=X,Y=y,k_ = 5, execs_ = 3, n_queries_ = 80, n_init_ = 10, train_sum_ = False, sep_class_ = True, \\\n",
    "             ml_method_ = 3, ml_sep_method_ = 3, al_method_ = 3, qbc_learners_ = [2], n_qbc_learners_ = 8, \\\n",
    "             weighting_factor_ = 0, save_figs_ = True, save_ratio_ = True, save_results_ = True, time_point = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
