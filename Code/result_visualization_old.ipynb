{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-9c082aee11e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# The name is the name of the file given to the produced file, typically being similar to the name of the file from which the frequencies were taken.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Target ids is the list of trajectory ids for positive instances, which allows identification of which trajectories are positive or negative.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mplot_heatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Remove trips from matrix for which, at no time point, they have been selected at least threshold amount of times\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmatrix_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatrix_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmatrix_\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Plot frequency selection heatmap from given dataframe. The matrix is the dataframe containing the frequencies for all trajectories\n",
    "# The threshold gives the minimum frequency required for a trajectory to be included in the heatmap. Bin size defines the size of bins in the heatmap.\n",
    "# The name is the name of the file given to the produced file, typically being similar to the name of the file from which the frequencies were taken.\n",
    "# Target ids is the list of trajectory ids for positive instances, which allows identification of which trajectories are positive or negative.\n",
    "def plot_heatmap(matrix_, threshold, bin_size, name, target_ids: np.array):\n",
    "    # Remove trips from matrix for which, at no time point, they have been selected at least threshold amount of times\n",
    "    matrix_ = matrix_.loc[:, (matrix_ >= threshold).any(axis=0)]\n",
    "    # Could divide numbers by total number of runs (executions+folds(-1?) to get better scaled numbers\n",
    "    # Might not be necessary, considering this is mostly used for finding interesting cases for one method at a time.\n",
    "\n",
    "    # Divide the remaining frequencies into bins\n",
    "    matrix = matrix_.T\n",
    "    matrix = matrix.groupby([[i//bin_size for i in range(0,matrix.shape[1])]], axis = 1).sum()\n",
    "    matrix.columns = [str(i*bin_size + 1) + \"-\" + str(i*bin_size+bin_size) for i in range(0,int(matrix.shape[1]))]\n",
    "    matrix = matrix.T\n",
    "\n",
    "    x_axis_labels = matrix_.columns # labels for x-axis\n",
    "    y_axis_labels = matrix.columns  # labels for y-axis\n",
    "    \n",
    "    plt.figure(figsize=(0.6*matrix.shape[1],0.6*matrix.shape[0]))\n",
    "    ax = plt.axes()\n",
    "    ax.set_title('Trajectory Selection Frequencies',fontsize=14, fontweight='bold')\n",
    "\n",
    "    ax.xticklabels = x_axis_labels\n",
    "    ax.yticklabels = y_axis_labels\n",
    "    \n",
    "    sn.heatmap(matrix, annot=False, cmap=\"Reds\", fmt='g', ax=ax, square = False)\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    \n",
    "    # Make all positive labels green, and negative labels red\n",
    "    for lab in ax.get_xticklabels():\n",
    "        text =  lab.get_text()\n",
    "        if text in str(target_ids):\n",
    "            lab.set_color('green')\n",
    "        else:\n",
    "            lab.set_color('red')\n",
    "\n",
    "    plt.savefig(\"../ResultFigs/\" + name + \"_heat.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# Test example\n",
    "# confusion_matrix = np.array([[41792,67554,19872,99459],[ 24901,11070,23452,15790],[20190,24793,34254,10582],[90190,24793,34254,20582]])\n",
    "# confusion_matrix = pd.DataFrame(confusion_matrix, columns = ['trip1', 'trip2', 'trip3', 'trip4'])\n",
    "\n",
    "# plot_heatmap(confusion_matrix, threshold = 1, bin_size = 2, name = \"Test_case\", target_ids = [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining and loading the desired matrix with frequencies of selected trajectories\n",
    "# MAKE SURE TO SWITCH ALSO LOAD THE OLD OR NEW DATA WHEN SWITCHING SO TRAJECTORY ID's CORRESPOND CORRECTLY\n",
    "\n",
    "# # Random sampling old data\n",
    "# string = \"random_sampling_OLD_same_sum_maj_UNDER_0.25_40_freq\"\n",
    "# traj_freq_matrix = pd.read_pickle(\"../Results/\" + string + \".pkl\")\n",
    "# plot_heatmap(traj_freq_matrix, threshold = 3, bin_size = 5, name = string, target_ids = looping_target_ids)\n",
    "\n",
    "# # Uncertainty sampling old data\n",
    "# string = \"uncertainty_sampling_WF0.2_OLD_same_sum_maj__40_freq\"\n",
    "# traj_freq_matrix = pd.read_pickle(\"../Results/\" + string + \".pkl\")\n",
    "# plot_heatmap(traj_freq_matrix, threshold = 4, bin_size = 5, name = string, target_ids = looping_target_ids)\n",
    "\n",
    "# MAKE SURE TO SWITCH ALSO LOAD THE OLD OR NEW DATA WHEN SWITCHING\n",
    "# Random sampling new data\n",
    "# string = \"random_sampling_OLD_same_sum_maj_UNDER_0.25_40_freq\"\n",
    "# traj_freq_matrix = pd.read_pickle(\"../Results/\" + string + \".pkl\")\n",
    "# plot_heatmap(traj_freq_matrix, threshold = 2, bin_size = 5, name = string, target_ids = looping_target_ids)\n",
    "\n",
    "# Uncertainty sampling new data\n",
    "# string = \"uncertainty_sampling_WF0.2_same_sum_maj__100_freq\"\n",
    "# traj_freq_matrix = pd.read_pickle(\"../Results/\" + string + \".pkl\")\n",
    "# plot_heatmap(traj_freq_matrix, threshold = 5, bin_size = 10, name = string, target_ids = looping_target_ids)\n",
    "\n",
    "result_strings = [\"random_sampling_same_sum_maj_100__new_freq\", \"uncertainty_sampling_WF0_same_sum_maj_100__new_freq\", \"density_sampling_WF0_same_sum_maj_100__new_freq\", \n",
    "                \"qbc_sampling_WF0_2_8_sep_sum_maj_100__new_freq\", \"pal_sampling_WF0_BD_0same_sum_maj_100__new_freq\"]\n",
    "thresholds = [3,6,8,14,3]\n",
    "for i, string in enumerate(result_strings):\n",
    "    traj_freq_matrix = pd.read_pickle(\"../Results/\" + string + \".pkl\")\n",
    "    plot_heatmap(traj_freq_matrix, threshold = thresholds[i], bin_size = 10, name = string, target_ids = looping_target_ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot frequency selection barchart from given dataframe. The matrix is the dataframe containing the frequencies for all trajectories\n",
    "# The threshold gives the minimum frequency required for a trajectory to be included in the heatmap.\n",
    "# The name is the name of the file given to the produced file, typically being similar to the name of the file from which the frequencies were taken.\n",
    "# Target ids is the list of trajectory ids for positive instances, which allows identification of which trajectories are positive or negative.\n",
    "def plot_barchart(matrix, threshold, name, target_ids: np.array):\n",
    "    matrix = matrix.loc[:, (matrix >= threshold).any(axis=0)] # .div(len(trajectory))\n",
    "    total_selection_frequencies_columns = matrix.columns.to_numpy().astype(str)\n",
    "    total_selection_frequencies_sums = matrix.sum(axis=0).to_numpy()\n",
    "\n",
    "    dims = (25, 10)\n",
    "    fig, ax = plt.subplots(figsize=dims)\n",
    "    plt.xticks(rotation=90)\n",
    "    \n",
    "    sn.barplot(x=total_selection_frequencies_columns, y=total_selection_frequencies_sums, ax=ax, dodge=False, color='blue')\n",
    "    \n",
    "    # make all positive labels green, and negative labels red\n",
    "    for lab in ax.get_xticklabels():\n",
    "        text =  lab.get_text()\n",
    "        if text in str(target_ids):\n",
    "            lab.set_color('green')\n",
    "        else:\n",
    "            lab.set_color('red')\n",
    "    \n",
    "    plt.savefig(\"../Results/\" + name + \"_bar.png\", bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and testing producing of bar charts\n",
    "\n",
    "result_strings = [\"random_sampling_same_sum_maj_100__new_freq\", \"uncertainty_sampling_WF0_same_sum_maj_100__new_freq\", \"density_sampling_WF0_same_sum_maj_100__new_freq\", \n",
    "                \"qbc_sampling_WF0_2_8_sep_sum_maj_100__new_freq\", \"pal_sampling_WF0_BD_0same_sum_maj_100__new_freq\"]\n",
    "\n",
    "thresholds = [3,6,8,10,3]\n",
    "for i, string in enumerate(result_strings):\n",
    "    traj_freq_matrix = pd.read_pickle(\"../Results/\" + string + \".pkl\")\n",
    "    plot_barchart(traj_freq_matrix, threshold = thresholds[i], name = string, target_ids = looping_target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method that takes selection frequency and gives the top_n most frequently selected trajectories. \n",
    "# The matrix is the dataframe containing the frequencies for all trajectories\n",
    "# The threshold gives the minimum frequency required for a trajectory to be included.\n",
    "# The name is the name of the file given to the produced file, typically being similar to the name of the file from which the frequencies were taken.\n",
    "# Target ids is the list of trajectory ids for positive instances, which allows identification of which trajectories are positive or negative.\n",
    "def freq_stats(matrix, threshold, name, target_ids: np.array, top_n):\n",
    "    matrix = matrix.loc[:, (matrix >= threshold).any(axis=0)] # .div(len(trajectory))\n",
    "    total_selection_frequencies_columns = matrix.columns.to_numpy().astype(str)\n",
    "    total_selection_frequencies_sums = matrix.sum(axis=0).to_numpy()\n",
    "    for i in range(top_n):\n",
    "        index = total_selection_frequencies_sums.argsort()[-1]\n",
    "        highest_value = total_selection_frequencies_sums[index]\n",
    "        all_hv_idx = np.where(total_selection_frequencies_sums == highest_value)\n",
    "        print(\"Rank \", i+1, \"highest value is: \", highest_value, \"for the following trajectories: \")\n",
    "        print(total_selection_frequencies_columns[all_hv_idx])\n",
    "        print()\n",
    "        total_selection_frequencies_columns = np.delete(total_selection_frequencies_columns, all_hv_idx)\n",
    "        total_selection_frequencies_sums = np.delete(total_selection_frequencies_sums, all_hv_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading and testing top N frequency production\n",
    "result_strings = [\"random_sampling_same_sum_maj_100__new_freq\", \"uncertainty_sampling_WF0_same_sum_maj_100__new_freq\", \"density_sampling_WF0_same_sum_maj_100__new_freq\", \n",
    "                \"qbc_sampling_WF0_2_8_sep_sum_maj_100__new_freq\", \"pal_sampling_WF0_BD_0same_sum_maj_100__new_freq\"]\n",
    "\n",
    "thresholds = [3,6,8,10,3]\n",
    "for i, string in enumerate(result_strings):\n",
    "    traj_freq_matrix = pd.read_pickle(\"../Results/\" + string + \".pkl\")\n",
    "    freq_stats(traj_freq_matrix, thresholds[i], name = string, target_ids = looping_target_ids, top_n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(measure_names, result_strings, save = False):\n",
    "    fig, ax = plt.subplots(2,2, figsize=(8.5, 6), dpi=200)\n",
    "    for i, measure in enumerate(measure_names):\n",
    "        if len(measure_names) == 4:\n",
    "            if i < 2:\n",
    "                x = 0\n",
    "            else:\n",
    "                x = 1\n",
    "            if i % 2 == 0:\n",
    "                y = 0\n",
    "            else:\n",
    "                y = 1\n",
    "        # Plot our performance over time.\n",
    "        title = str()\n",
    "        names = []\n",
    "        plt.figure()\n",
    "        color = cm.rainbow(np.linspace(0, 1, len(result_strings)))\n",
    "        for c, result_string in enumerate(result_strings):\n",
    "            results = pd.read_pickle(\"../Results/\" + result_string + measure + \".pkl\")\n",
    "            mean_performance = results.describe().loc[['mean'], :].to_numpy()[0]\n",
    "            lower_quartiles = results.describe().loc[['25%'], :].to_numpy()[0]\n",
    "            upper_quartiles = results.describe().loc[['75%'], :].to_numpy()[0]\n",
    "            medians = results.describe().loc[['50%'], :].to_numpy()[0]\n",
    "\n",
    "            quartiles = list(zip(lower_quartiles,upper_quartiles))\n",
    "            \n",
    "            \n",
    "            ax[x,y].plot(mean_performance, c = color[c])\n",
    "            if measure != \"Selected Label Ratio\":\n",
    "                ax[x,y].plot((range(len(mean_performance)),range(len(mean_performance))),([i for (i,j) in quartiles], [j for (i,j) in quartiles]),c = color[c], label='_nolegend_', alpha = 0.5)\n",
    "                ax[x,y].plot(range(len(mean_performance)), [i for (i,j) in quartiles], '_', markersize = 6,c = color[c], label='_nolegend_')\n",
    "                ax[x,y].plot(range(len(mean_performance)), [j for (i,j) in quartiles], '_', markersize = 6,c = color[c], label='_nolegend_')\n",
    "            \n",
    "            ax[x,y].scatter(range(len(mean_performance)), mean_performance, s=13,c=\"blue\", marker = \"\")\n",
    "#             method_name = result_string.split(\"_\")[0] + \" \" + result_string.split(\"_\")[1]\n",
    "            method_name = result_string.split(\"_\")[0] + \" \" + result_string.split(\"_\")[3]  + \" \" + result_string.split(\"_\")[4]\n",
    "#             if c == 1:\n",
    "#                 method_name = \"undersampled \" + result_string.split(\"_\")[0] + \" \" + result_string.split(\"_\")[1]\n",
    "            names.append(method_name)\n",
    "            title = title + \"\\n\" + method_name +  \" & \"\n",
    "\n",
    "        ax[x,y].xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))\n",
    "        ax[x,y].yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))\n",
    "        ax[x,y].yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n",
    "\n",
    "        if measure != \"Selected Label Ratio\":\n",
    "            ax[x,y].set_ylim(bottom = 0.45, top=1) # bottom=0, top=1\n",
    "            ax[x,y].grid(True)\n",
    "        else:\n",
    "            ax[x,y].set_ylim(bottom=0, top=1) # bottom=0, top=1\n",
    "            ax[x,y].grid(True)\n",
    "        title = title[:len(title)-2]\n",
    "        ax[x,y].set_title('Incremental Classification ' + measure + \" \" + title, fontsize = 6)\n",
    "        ax[x,y].set_xlabel('Query iteration', fontsize = 6)\n",
    "        ax[x,y].set_ylabel(measure, fontsize = 6)\n",
    "        ax[x,y].legend(names, fontsize = 5)\n",
    "\n",
    "        fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n",
    "                            hspace=0.4, wspace=0.3)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig1 = plt.gcf()\n",
    "    plt.show()\n",
    "    plt.draw()\n",
    "    if save:\n",
    "        filename = '_'.join(names)\n",
    "        string = \"../ResultFigs/\" + filename + \"_new\"  + \".png\"\n",
    "        fig1.savefig(string, bbox_inches='tight')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ALC value given a set of AUC scores over the course of training\n",
    "def calc_alc(auc_row):\n",
    "    \n",
    "    x = range(len(auc_row))\n",
    "    x = x[1:]\n",
    "    y = auc_row\n",
    "    rand_predict = auc_row[0]\n",
    "    y = auc_row[1:]\n",
    "\n",
    "    x = np.log2(x)\n",
    "    A=trapz(y)\n",
    "    Arand = rand_predict * x[-1]\n",
    "    Amax = x[-1]\n",
    "\n",
    "    global_score = (A-Arand)/(Amax-Arand)\n",
    "\n",
    "#     print(\"ALC is: \", global_score)\n",
    "    return(global_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_invalid_red(val):\n",
    "    color = 'red' if val > 0.05 else 'black'\n",
    "    return 'color: %s' % color\n",
    "\n",
    "# Perform the Wilcoxon signed-rank test for the ALC scores of different methods. Make sure the number of runs and queries per run are the same\n",
    "def wsrt(result_strings, save = False):\n",
    "    method_names = np.empty(len(result_strings), dtype='U100')\n",
    "    results_table = np.empty((0,len(result_strings)), dtype = 'float')\n",
    "    for i, string in enumerate(result_strings):\n",
    "        method_names[i] = string.split(\"_\")[0]\n",
    "\n",
    "    for result_string1 in result_strings:\n",
    "        results1 = pd.read_pickle(\"../Results/\" + result_string1 + \"AUC.pkl\")\n",
    "        alc_scores_1 = results1.apply(calc_alc, axis=1)\n",
    "        results_array = np.empty(len(result_strings))\n",
    "        \n",
    "        for j, result_string2 in enumerate(result_strings):\n",
    "            if result_string1 != result_string2:\n",
    "                results2 = pd.read_pickle(\"../Results/\" + result_string2 + \"AUC.pkl\")\n",
    "                alc_scores_2 = results2.apply(calc_alc, axis=1)\n",
    "                p_value = wilcoxon(alc_scores_1, alc_scores_2, correction=False)\n",
    "                results_array[j] = p_value[1]\n",
    "#                 print(\"For \", result_string1, \" and \", result_string2, \" the p-value given the ALC scores is :\\n\", p_value[1], \"\\n\")\n",
    "            else:\n",
    "                results_array[j] = float('NaN')\n",
    "#             print(results_array[i])\n",
    "                \n",
    "#         print(results_array)\n",
    "#         np.append(arr, np.array([[1,2,3]]), axis=0)\n",
    "#         results_table = np.append(results_table, results_array, axis=0)\n",
    "        results_table = np.vstack((results_table, results_array))\n",
    "    results_table = pd.DataFrame(results_table, index = method_names, columns = method_names)\n",
    "#     pd.options.display.float_format = '{:.2e}'.format\n",
    "#     pd.options.display.float_format = '{:.2f}'.format\n",
    "#     pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "#     pd.set_option('display.float_format', lambda x: f'{x:,.3f}')\n",
    "    with pd.option_context('display.max_rows', 5, 'display.max_columns', 5):\n",
    "        pd.set_option('display.float_format', '{:.2e}'.format)\n",
    "#         display(results_table)\n",
    "        pd.set_option(\"display.precision\", 1)\n",
    "        results_table.style\n",
    "        display(results_table.style.applymap(color_invalid_red).format('{:.2E}', na_rep='NA'))\n",
    "        \n",
    "        config = imgkit.config(wkhtmltoimage='/data/sammeyer/.conda/envs/AL_project/bin/wkhtmltoimage') #, xvfb='/opt/bin/xvfb-run'\n",
    "        html = results_table.style.set_properties(**{'background-color': 'ghostwhite',                                                   \n",
    "                                        'color': 'black',                       \n",
    "                                        'border-color': 'white'}).render()\n",
    "        imgkit.from_string(html, '../ResultFigs/wsrt_table.png', config=config)\n",
    "    \n",
    "    \n",
    "#     if save:\n",
    "#         string = \"../ResultFigs/\" + name + \"_\" + \"wsrt_table\" + \".txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the Wilcoxon signed-rank test for the ALC scores of different methods. Make sure the number of runs and queries per run are the same\n",
    "def average_ALC(result_arrays, save = False):\n",
    "    method_names = np.empty(len(result_arrays[0]), dtype='U100')\n",
    "    columns = [\"Comp.\", \"10h\", \"16h\", \"20h\", \"25h\", \"30h\", \"35h\", \"40h\"]\n",
    "    \n",
    "    for i, string in enumerate(result_arrays[0]):\n",
    "        method_names[i] = string.split(\"_\")[0]\n",
    "        \n",
    "    results_table = pd.DataFrame(columns = columns, index = method_names, dtype = 'float')\n",
    "    \n",
    "    for j, result_strings in enumerate(result_arrays):\n",
    "        for k, result_string in enumerate(result_strings):\n",
    "            results = pd.read_pickle(\"../Results/\" + result_string + \"AUC.pkl\")\n",
    "            alc_score = np.average(results.apply(calc_alc, axis=1).to_numpy())\n",
    "            results_table.loc[method_names[k], columns[j]] = alc_score\n",
    "        \n",
    "#     pd.options.display.float_format = '{:.2e}'.format\n",
    "#     pd.options.display.float_format = '{:.2f}'.format\n",
    "#     pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "#     pd.set_option('display.float_format', lambda x: f'{x:,.3f}')\n",
    "    with pd.option_context('display.max_rows', 99, 'display.max_columns', 99):\n",
    "#         pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "#         display(results_table)\n",
    "#         pd.set_option(\"display.precision\", 1)\n",
    "        results_table.style\n",
    "        display(results_table.style.format('{:.2f}', na_rep='NaN'))\n",
    "        \n",
    "#         config = imgkit.config(wkhtmltoimage='/data/sammeyer/.conda/envs/AL_project/bin/wkhtmltoimage') #, xvfb='/opt/bin/xvfb-run'\n",
    "#         html = results_table.style.set_properties(**{'background-color': 'ghostwhite',                                                   \n",
    "#                                         'color': 'black',                       \n",
    "#                                         'border-color': 'white'}).render()\n",
    "#         imgkit.from_string(html, '../ResultFigs/wsrt_table.png', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "measure_names = ['F1', 'Recall', 'Precision', \"Selected Label Ratio\", \"AUC\"]\n",
    "\n",
    "result_strings = [[\"random_sampling_same_sum_maj_100__new_\", \"uncertainty_sampling_WF0_same_sum_maj_100__new_\", \"density_sampling_WF0_same_sum_maj_100__new_\", \n",
    "                  \"qbc_sampling_WF0_2_8_sep_sum_maj_100__new_\", \"xpal_sampling_WF0_BD_0same_sum_maj_100__new_\"],[\"random_sampling_same_sum_maj_100_10__new_\",\n",
    "                  \"uncertainty_sampling_WF0_same_sum_maj_100_10__new_\", \"density_sampling_WF0_same_sum_maj_100_10__new_\", \"qbc_sampling_WF0_2_8_sep_sum_maj_100_10__new_\",\n",
    "                 \"xpal_sampling_WF0_BD_0same_sum_maj_100_10__new_\"], [\"random_sampling_same_sum_maj_100_16__new_\", \"uncertainty_sampling_WF0_same_sum_maj_100_16__new_\", \n",
    "                  \"density_sampling_WF0_same_sum_maj_100_16__new_\", \"qbc_sampling_WF0_2_8_sep_sum_maj_100_16__new_\",\"xpal_sampling_WF0_BD_0same_sum_maj_100_16__new_\"],\n",
    "                  [\"random_sampling_same_sum_maj_100_20__new_\", \"uncertainty_sampling_WF0_same_sum_maj_100_20__new_\", \"density_sampling_WF0_same_sum_maj_100_20__new_\", \n",
    "                   \"xpal_sampling_WF0_BD_0same_sum_maj_100_20__new_\"], [\"random_sampling_same_sum_maj_100_25__new_\", \"uncertainty_sampling_WF0_same_sum_maj_100_25__new_\", \n",
    "                    \"density_sampling_WF0_same_sum_maj_100_25__new_\", \"qbc_sampling_WF0_2_8_sep_sum_maj_100_25__new_\", \"xpal_sampling_WF0_BD_0same_sum_maj_100_25__new_\"],\n",
    "                  [\"random_sampling_same_sum_maj_100_30__new_\", \"uncertainty_sampling_WF0_same_sum_maj_100_30__new_\", \"density_sampling_WF0_same_sum_maj_100_30__new_\", \n",
    "                   \"qbc_sampling_WF0_2_8_sep_sum_maj_100_30__new_\",\"xpal_sampling_WF0_BD_0same_sum_maj_100_30__new_\"], [\"random_sampling_same_sum_maj_100_35__new_\", \n",
    "                  \"uncertainty_sampling_WF0_same_sum_maj_100_35__new_\", \"density_sampling_WF0_same_sum_maj_100_35__new_\", \"qbc_sampling_WF0_2_8_sep_sum_maj_100_35__new_\",\n",
    "                 \"xpal_sampling_WF0_BD_0same_sum_maj_100_35__new_\"], [\"random_sampling_same_sum_maj_100_40__new_\", \"uncertainty_sampling_WF0_same_sum_maj_100_40__new_\", \n",
    "                \"density_sampling_WF0_same_sum_maj_100_40__new_\", \"qbc_sampling_WF0_2_8_sep_sum_maj_100_40__new_\", \"xpal_sampling_WF0_BD_0same_sum_maj_100_40__new_\"]]\n",
    "\n",
    "average_ALC(result_strings, save = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
