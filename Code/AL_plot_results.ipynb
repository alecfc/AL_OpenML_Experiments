{"cells":[{"cell_type":"markdown","metadata":{},"source":["from pip command"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name '__file__' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-1-5b370bcaeb35>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimgkit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrealpath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name '__file__' is not defined"]}],"source":["import os\n","import sys\n","import imgkit\n","sys.path.append(os.path.dirname(os.path.realpath(__file__)))\n","import pandas as pd\n","import seaborn as sn\n","import matplotlib.pyplot as plt\n","from IPython.display import display\n","import matplotlib as mpl\n","import numpy as np\n","import plotly.graph_objects as go\n","from scipy.stats import wilcoxon"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from AL_methods import *\n"]},{"cell_type":"markdown","metadata":{},"source":["from pip command"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def init_set_generator(prob_ratio, size_initial):\n","    \"\"\"\"\n","    Generates initial set with size and ratio, used for replotting results.\n","    \"\"\"\n","    number_class_1 = int(round(prob_ratio * size_initial))\n","    number_class_0 = int(size_initial - number_class_1)\n","    list_zeroes = np.zeros((number_class_0,), dtype=int)\n","    list_ones = np.ones((number_class_1,), dtype=int)\n","    return list(list_zeroes) + list(list_ones)"]},{"cell_type":"markdown","metadata":{},"source":["Method for plotting the saved results as figures."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_results(X, results_, measure_name_, ML_results_fully_trained_, name_, al_method_, ml_method_, save_=False,\n","                 normalize_data_=False,\n","                 prop_performance_=False, file_path_='../Figures/', data_title_='', al_dict_=AL_switcher):\n","    \"\"\"\"\n","    Plots results for a single performance metric, using mean performance and lower and upper quartiles.\n","    \"\"\"\n","    mean_performance = results_.describe().loc[['mean'], :].to_numpy()[0]\n","    lower_quartiles = results_.describe().loc[['25%'], :].to_numpy()[0]\n","    upper_quartiles = results_.describe().loc[['75%'], :].to_numpy()[0]\n","    medians = results_.describe().loc[['50%'], :].to_numpy()[0]\n","    sn.set_theme()\n","    # Plot our performance over time.\n","    fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n","    if normalize_data_:\n","        num_rows, num_cols = X.shape\n","        plot_range = np.arange(len(mean_performance)) / num_rows\n","        ax.set_xlabel('Query iteration/dataset size')\n","    else:\n","        ax.set_xlabel('Query iteration')\n","        plot_range = range(len(mean_performance))\n","        if measure_name_ == \"Loss Difference\":\n","            ax.plot(mean_performance)\n","            quartiles = list(zip(lower_quartiles, upper_quartiles))\n","            plt.plot((plot_range, plot_range), ([i for (i, j) in quartiles], [j for (i, j) in quartiles]), c='black')\n","            plt.plot(plot_range, [i for (i, j) in quartiles], '_', markersize=6, c='blue')\n","            plt.plot(plot_range, [j for (i, j) in quartiles], '_', markersize=6, c='blue')\n","            ax.set_xlabel('Query iteration')\n","            ax.set_ylabel('Active learning bias through difference in estimated risk')\n","            ax.set_title(\n","                data_title_ + ': Bias Through Difference in Risk with Fully Trained ' + type(ML_switcher[ml_method_]).__name__ + ' Classifier')\n","            if save_:\n","                string = file_path_ + name_ + '_' + '.png'\n","                plt.savefig(string, bbox_inches='tight')\n","            return\n","        if measure_name_ == \"Label Ratio\":\n","            ax.plot(mean_performance)\n","        if prop_performance_ and measure_name_ != \"Label Ratio\":\n","            result_dict = ML_results_fully_trained_[ml_method_]\n","            mean_performance = result_dict[measure_name_] / mean_performance\n","            lower_quartiles = result_dict[measure_name_] / lower_quartiles\n","            upper_quartiles = result_dict[measure_name_] / upper_quartiles\n","            plot_range = range(len(mean_performance))\n","            ax.plot(mean_performance)\n","            ax.set_ylabel('Propotional to fully trained classifier ' + measure_name_)\n","            quartiles = list(zip(lower_quartiles, upper_quartiles))\n","            plt.plot((plot_range, plot_range), ([i for (i, j) in quartiles], [j for (i, j) in quartiles]), c='black')\n","            plt.plot(plot_range, [i for (i, j) in quartiles], '_', markersize=6, c='blue')\n","            plt.plot(plot_range, [j for (i, j) in quartiles], '_', markersize=6, c='blue')\n","            #plt.show()\n","            return\n","    ax.set_xlabel('Query iteration')\n","    ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))\n","    ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n","    quartiles = list(zip(lower_quartiles, upper_quartiles))\n","    ax.scatter(plot_range, mean_performance, s=13, c=\"blue\")\n","    ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))\n","    if measure_name_ == \"Label Ratio\":\n","        ax.set_ylim(bottom=0, top=1, auto=False)\n","    else:\n","        ax.set_ylim(bottom=0.4, top=1, auto=False)  # bottom=0, top=1\n","    ax.grid(True)\n","    ax.set_title(data_title_ + ' Incremental Classification ' + measure_name_ + \": \" + al_dict_[\n","        al_method_].__name__ + ' using ' + type(ML_switcher[ml_method_]).__name__ + ' classifier')\n","    ax.set_ylabel('Classification ' + measure_name_)\n","    #     ax.legend(loc=\"lower right\")\n","    if measure_name_ == 'AUC':\n","        alc = calc_alc(pd.Series(mean_performance))\n","        print(\"ALC is: \", alc)\n","        if save_:\n","            string = file_path_ + name_ + \"_\" + \"ALC\" + \".txt\"\n","            text_file = open(string, \"w\")\n","            n = text_file.write(str(alc))\n","            text_file.close()\n","    if measure_name_ != \"Label Ratio\":\n","        plt.plot((plot_range, plot_range), ([i for (i, j) in quartiles], [j for (i, j) in quartiles]), c='black')\n","        plt.plot(plot_range, [i for (i, j) in quartiles], '_', markersize=6, c='blue')\n","        plt.plot(plot_range, [j for (i, j) in quartiles], '_', markersize=6, c='blue')\n","    fig.subplots_adjust(left=0.08, right=0.98, bottom=0.05, top=0.9,\n","                        hspace=0.4, wspace=0.3)\n","    if save_:\n","        string = file_path_ + name_ + '.png'\n","        plt.savefig(string, bbox_inches='tight')\n","    # plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["In[15]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_3d_results(results_, metric_name_, save_, file_path_, z_labels_, experiment_type_, dataset_name_):\n","    \"\"\"\"\n","    Plots 3D results for multiple settings of an experiment.\n","    \"\"\"\n","    sn.set_theme()\n","    data = []\n","    title = '3D Comparison of ' + metric_name_ + ' for the ' + experiment_type_ + ' experiment on the ' + dataset_name_ + ' dataset'\n","    for idx, result in enumerate(results_):\n","        mean_performance = result.describe().loc[['mean'], :].to_numpy()[0]\n","        categories = \"50-50 \" * len(mean_performance)\n","        categories = categories.split(\" \")\n","        categories.pop(len(mean_performance))\n","        plot_range = range(len(mean_performance))\n","        df = pd.DataFrame({\n","            'cat': categories, 'Number of Queries': plot_range, 'Performance': mean_performance\n","        })\n","        df['Experiment Category'] = z_labels_[idx]\n","        trace = go.Scatter3d(x=df['Number of Queries'],\n","                             y=df['Experiment Category'],\n","                             z=df['Performance'],\n","                             name=z_labels_[idx],\n","                             mode='markers')\n","        data.append(trace)\n","\n","    # style layout\n","    layout = go.Layout(\n","        title=title,\n","        scene=dict(\n","            xaxis_title='Number of Queries',\n","            yaxis_title='Class Ratio',\n","            zaxis_title=metric_name_,\n","        ),\n","    )\n","    fig = go.Figure(layout=layout, data=data)\n","    if save_:\n","        string = file_path_ + title + '.png'\n","        fig.write_image(string)\n","    # fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["In[16]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_multiple(results_, metric_name_, setting_names_, experiment_type_, save_, file_path_, dataset_name_):\n","    \"\"\"\"\n","    Plots multiple settings in single graph.\n","    \"\"\"\n","    sn.set_theme()\n","    data = []\n","    colors = ['#B42B2D', '#0E84FA', '#FAAB0E', '#121110', '#249338', '#894CB6']\n","    fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n","    title = \"Comparison of \" + metric_name_ + \" for the \" + experiment_type_ + ' experiment on ' + dataset_name_\n","    ax.set_title(title)\n","    ax.set_ylabel('Classification ' + metric_name_)\n","    ax.set_xlabel('Query iteration')\n","    if metric_name_ == \"Label Ratio\":\n","        ax.set_ylim(bottom=0, top=1, auto=False)\n","    else:\n","        if metric_name_ != \"Loss Difference\":\n","            ax.set_ylim(bottom=0.4, top=1, auto=False)  # bottom=0, top=1\n","    ax.grid(True)\n","    if metric_name_ != \"Loss Difference\":\n","        ax.yaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=10))\n","        ax.yaxis.set_major_formatter(mpl.ticker.PercentFormatter(xmax=1))\n","        ax.xaxis.set_major_locator(mpl.ticker.MaxNLocator(nbins=5, integer=True))\n","    for idx, result in enumerate(results_):\n","        mean_performance = result.describe().loc[['mean'], :].to_numpy()[0]\n","        lower_quartiles = result.describe().loc[['25%'], :].to_numpy()[0]\n","        upper_quartiles = result.describe().loc[['75%'], :].to_numpy()[0]\n","        if metric_name_ != \"Label Ratio\":\n","            for j, quartile_result in enumerate(lower_quartiles):\n","                if j % 5 != 0:\n","                    upper_quartiles[j] = mean_performance[j]\n","                    lower_quartiles[j] = mean_performance[j]\n","        medians = result.describe().loc[['50%'], :].to_numpy()[0]\n","        plot_range = range(len(mean_performance))\n","        quartiles = list(zip(lower_quartiles, upper_quartiles))\n","        ax.plot(mean_performance, c=colors[idx])\n","        ax.scatter(plot_range, mean_performance, s=13, c=colors[idx], label=setting_names_[idx])\n","        if metric_name_ != \"Label Ratio\":\n","            plt.plot((plot_range, plot_range), ([i for (i, j) in quartiles], [j for (i, j) in quartiles]), c=colors[idx])\n","            plt.plot(plot_range, [i for (i, j) in quartiles], '_', markersize=6, c=colors[idx])\n","            plt.plot(plot_range, [j for (i, j) in quartiles], '_', markersize=6, c=colors[idx])\n","    if metric_name_ == \"Label Ratio\" or metric_name_ == \"Loss Difference\":\n","        ax.legend(loc='upper right')\n","    else:\n","        ax.legend(loc='lower right')\n","    if save_:\n","        string = file_path_ + title + '.png'\n","        plt.savefig(string, bbox_inches='tight')\n","    # fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["In[17]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_risk_difference(results_, loss_fully_trained_, model_name_, save_, file_path_, name_, dataset_name_):\n","    \"\"\"\"\n","    Plot for risk difference through difference in log loss over queries.\n","    \"\"\"\n","    # Plot bias through difference of estimated risk (loss)\n","    sn.set_theme()\n","    fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n","    mean_performance = results_.describe().loc[['mean'], :].to_numpy()[0]\n","    lower_quartiles = results_.describe().loc[['25%'], :].to_numpy()[0]\n","    upper_quartiles = results_.describe().loc[['75%'], :].to_numpy()[0]\n","    medians = results_.describe().loc[['50%'], :].to_numpy()[0]\n","    plot_range = range(len(mean_performance))\n","    ax.set_xlabel('Query iteration')\n","    ax.set_ylabel('Active learning bias through difference in estimated risk')\n","    mean_performance = loss_fully_trained_ - mean_performance\n","    lower_quartiles = loss_fully_trained_ - lower_quartiles\n","    upper_quartiles = loss_fully_trained_ - upper_quartiles\n","    ax.plot(mean_performance)\n","    quartiles = list(zip(lower_quartiles, upper_quartiles))\n","    ax.set_title(dataset_name_ + ': Bias Through Difference in Risk with Fully Trained ' + model_name_ + ' Classifier')\n","    plt.plot((plot_range, plot_range), ([i for (i, j) in quartiles], [j for (i, j) in quartiles]), c='black')\n","    plt.plot(plot_range, [i for (i, j) in quartiles], '_', markersize=6, c='blue')\n","    plt.plot(plot_range, [j for (i, j) in quartiles], '_', markersize=6, c='blue')\n","    if save_:\n","        string = file_path_ + name_ + '.png'\n","        plt.savefig(string, bbox_inches='tight')\n","    # plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["In[18]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_multiple_bias(initial_labels_, labels_, original_class_ratio_, save_, file_path_, setting_names_, experiment_type_, dataset_name_):\n","    \"\"\"\"\n","    Plots bias through class ratio difference for multiple settings of a single experiment.\n","    \"\"\"\n","    fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n","    sn.set_theme()\n","    colors = ['#B42B2D', '#0E84FA', '#FAAB0E', '#121110', '#249338', '#894CB6']\n","    ax.set_xlabel('Query iteration')\n","    ax.set_ylabel('Active learning bias through class ratio difference')\n","    title = \"Comparison of Bias Through Class Ratio Difference Using Original Ratio of \" + str(original_class_ratio_) \\\n","            + \" for the \" + experiment_type_ + \" experiment on \" + dataset_name_\n","    if experiment_type_ == 'Class Imbalance':\n","        class_ratios = [0.25, 0.05, 0.5, original_class_ratio_]\n","        original_class_ratio_ = class_ratios\n","    elif experiment_type_ == 'Initial Class Ratio':\n","        init_ratios = [0.1, 0.25, 0.5]\n","    ax.set_title(\n","        dataset_name_ + ': Comparison of Bias Through Class Ratio Difference Using Original Ratio of ' + str(original_class_ratio_))\n","    for idx, label in enumerate(labels_):\n","        top_selected_labels = label.mode()\n","        plot_range = range(len(label.columns))\n","        ratio_differences = []\n","        if experiment_type_ == 'Initial Class Ratio':\n","            initial_labels_ = init_set_generator(init_ratios[idx],10)\n","        current_labels = initial_labels_\n","        if experiment_type_ == 'Class Imbalance':\n","            original_class_ratio_ = class_ratios[idx]\n","        for i in top_selected_labels.to_numpy()[0]:\n","            updated_class_ratio = round(\n","                Counter(current_labels)[1] / (Counter(current_labels)[0] + Counter(current_labels)[1]), 2)\n","            ratio_differences.append(original_class_ratio_ - updated_class_ratio)\n","            current_labels = np.append(current_labels, [i])\n","        plt.plot(plot_range, [i for i in ratio_differences], c=colors[idx], label=setting_names_[idx])\n","    plt.legend(loc=\"lower right\")\n","    if save_:\n","        string = file_path_ + title + '.png'\n","        plt.savefig(string, bbox_inches='tight')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_bias(initial_labels_, labels_, original_class_ratio_, save_, file_path_, name_, dataset_name_):\n","    \"\"\"\"\n","    Plots bias through class ratio difference between labelled dataset over queries and class ratio of original dataset.\n","    \"\"\"\n","    # Plot our bias difference over time, through calculating the difference between class ratio's\n","    fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n","    sn.set_theme()\n","    top_selected_labels = labels_.mode()\n","    plot_range = range(len(labels_.columns))\n","    ratio_differences = []\n","    current_labels = initial_labels_\n","    for label in top_selected_labels.to_numpy()[0]:\n","        updated_class_ratio = round(\n","            Counter(current_labels)[1] / (Counter(current_labels)[0] + Counter(current_labels)[1]), 2)\n","        ratio_differences.append(original_class_ratio_ - updated_class_ratio)\n","        current_labels = np.append(current_labels, [label])\n","    ax.set_xlabel('Query iteration')\n","    ax.set_ylabel('Active learning bias through class ratio difference')\n","    ax.set_title(\n","        dataset_name_ + ': Bias Through Class Ratio Difference Using Original Ratio of ' + str(original_class_ratio_))\n","    plt.plot(plot_range, [i for i in ratio_differences], c='blue')\n","    if save_:\n","        string = file_path_ + name_ + '.png'\n","        plt.savefig(string, bbox_inches='tight')\n","    # plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_class_per_sample(labels_, save_, name_, file_path_, dataset_name_, al_method_, ml_method_, al_dict_=AL_switcher):\n","    \"\"\"\"\n","    Plots proportion of selected classes per query.\n","    \"\"\"\n","    sn.set_theme()\n","    proportion_per_query = []\n","    num_zeroes = []\n","    num_ones =[]\n","    total_per_exec = len(labels_[0])\n","    top_selected_labels = labels_.mode().to_numpy()[0]\n","    for (columnName, columnData) in labels_.iteritems():\n","        num_zeroes.append(columnData.loc[columnData < 1].count()/total_per_exec)\n","        num_ones.append(columnData.loc[columnData == 1].count()/total_per_exec)\n","    queries = list(range(1,len(labels_.T[0])+1))\n","    for j, quartile_result in enumerate(queries):\n","        if (j+1) % 5 != 0:\n","            queries[j] = ''\n","    df = pd.DataFrame(list(zip(num_zeroes, num_ones)), index=queries, columns=['Majority (Negative) Class', 'Minority (Positive) Class'], )\n","    (df*100).plot.bar(title=dataset_name_ + ': Proportion of Selected Classes per Query using ' + al_dict_[\n","        al_method_].__name__ + ' and ' + type(ML_switcher[ml_method_]).__name__ + ' classifier', stacked=True, figsize=(18, 6))\n","    plt.legend(loc='upper right')\n","    plt.xlabel('Query Iteration')\n","    plt.ylabel('Percentage of Chosen Classes')\n","    if save_:\n","        string = file_path_ + name_ + '.png'\n","        plt.savefig(string, bbox_inches='tight')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_top_selected_instances(instances, labels, save_, file_path_, name_):\n","    \"\"\"\"\n","    Plots top selected instances throughout multiple experiment runs.\n","    \"\"\"\n","    all_instances = []\n","    for column in instances:\n","        query_iteration_instances = instances[column].tolist()\n","        all_instances += query_iteration_instances\n","    all_labels = []\n","    for column in labels:\n","        query_iteration_labels = labels[column].tolist()\n","        all_labels += query_iteration_labels\n","    df_instances = pd.DataFrame(all_instances)\n","    df_instances['Label'] = pd.DataFrame(all_labels).round()\n","    fig, ax = plt.subplots(figsize=(8.5, 6), dpi=130)\n","    instance_frequencies = df_instances[0].value_counts().to_frame()\n","    instance_frequencies.columns = ['Amount of times queried']\n","    instance_frequencies['Instance number'] = instance_frequencies.index\n","    instance_frequencies['Label'] = np.nan\n","    instance_frequencies.reset_index()\n","    for idx, instance in enumerate(instance_frequencies['Instance number']):\n","        label_index = df_instances.index[df_instances[0] == instance].tolist()[0]\n","        instance_frequencies['Label'].iloc[idx] = df_instances['Label'].iloc[label_index]\n","    instance_frequencies['Instance number'] = instance_frequencies['Instance number'].round()\n","    instance_frequencies = instance_frequencies.astype({'Instance number': 'int'})\n","    sn.set_theme()\n","    ax = sn.barplot(x='Instance number', y=\"Amount of times queried\", hue=\"Label\", data=instance_frequencies.head(15))\n","    if save_:\n","        string = file_path_ + name_ + '_' + '.png'\n","        plt.savefig(string, bbox_inches='tight')\n","    # plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["In[20]:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_aggregate_results(experiment_name, aggregate_results, al_method_, ml_method_, al_switcher_):\n","    \"\"\"\"\n","    Plots average graphs for aggregate performance metric results.\n","    \"\"\"\n","    print('Aggregate Results:')\n","    for experiment_type, experiment_results in aggregate_results.items():\n","        experiment_title = experiment_type\n","        if experiment_name == 'AL_Methods':\n","            al_method_ =  [k for k, v in al_switcher_.items() if v.__name__ == experiment_title][0]\n","        if experiment_name == 'ML_Methods':\n","            ml_method_ =  [k for k, v in ML_switcher.items() if type(v).__name__ == experiment_title][0]\n","        file_path = \"../Figures/\" + experiment_name + \"/Aggregate_Results/\" + experiment_title + '/'\n","        if not os.path.exists(file_path):\n","            os.makedirs(file_path)\n","        for performance_metric_name, performance in aggregate_results[experiment_type].items():\n","            if experiment_name == 'Class_Imbalance':\n","                file_name = 'All Datasets Aggregate ' + performance_metric_name + ' Results for ' + experiment_title + ' Class Ratio'\n","            else:\n","                file_name = 'All Datasets Aggregate ' + performance_metric_name + ' Results for ' + experiment_title\n","            plot_results([], performance, performance_metric_name, False, file_name, al_method_, ml_method_, save_=True,\n","                         normalize_data_=False, prop_performance_=False, file_path_=file_path,\n","                         data_title_='Aggregate OpenML', al_dict_=al_switcher_)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_aggregate_comparison(experiment_name, aggregate_ci_results):\n","    \"\"\"\"\n","    Plots comparison of multiple settings for average aggregate performance metric results.\n","    \"\"\"\n","    print('Comparison of Experiment Settings on Aggregate')\n","    aggregate_list = list(aggregate_ci_results)\n","    stored_performance = aggregate_ci_results[aggregate_list[0]].items()\n","    methods = []\n","    if experiment_name == 'AL_Methods':\n","        for idx, subset_number in enumerate(aggregate_list):\n","            methods.append(subset_number)\n","        aggregate_list = methods\n","    elif experiment_name == 'ML_Methods':\n","        for idx, subset_number in enumerate(aggregate_list):\n","            methods.append(subset_number)\n","        aggregate_list = methods\n","    for performance_metric_name, performance in stored_performance:\n","        file_path = \"../Figures/\" + experiment_name + \"/Aggregate_Results/\" + performance_metric_name + '/'\n","        if not os.path.exists(file_path):\n","            os.makedirs(file_path)\n","        results_for_performance_metric = []\n","        for experiment_type, experiment_results in aggregate_ci_results.items():\n","            for experiment_performance_name, experiment_performance in aggregate_ci_results[experiment_type].items():\n","                if experiment_performance_name == performance_metric_name:\n","                    results_for_performance_metric.append(experiment_performance)\n","        plot_multiple(results_for_performance_metric, performance_metric_name, setting_names_=aggregate_list,\n","                      experiment_type_=experiment_name,\n","                      save_=True, file_path_=file_path, dataset_name_='aggregate')"]},{"cell_type":"markdown","metadata":{},"source":["Plot frequency selection heatmap from given dataframe. The matrix is the dataframe containing the frequencies for all trajectories<br>\n","The threshold gives the minimum frequency required for a trajectory to be included in the heatmap. Bin size defines the size of bins in the heatmap.<br>\n","The name is the name of the file given to the produced file, typically being similar to the name of the file from which the frequencies were taken.<br>\n","Target ids is the list of trajectory ids for positive instances, which allows identification of which trajectories are positive or negative."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_heatmap(matrix_, threshold, bin_size, name, target_ids: np.array):\n","    # Remove trips from matrix for which, at no time point, they have been selected at least threshold amount of times\n","    matrix_ = matrix_.loc[:, (matrix_ >= threshold).any(axis=0)]\n","    print(matrix_)\n","    # Could divide numbers by total number of runs (executions+folds(-1?) to get better scaled numbers\n","    # Might not be necessary, considering this is mostly used for finding interesting cases for one method at a time.\n","\n","    # Divide the remaining frequencies into bins\n","    matrix = matrix_.T\n","    matrix = matrix.groupby([[i // bin_size for i in range(0, matrix.shape[1])]], axis=1).sum()\n","    matrix.columns = [str(i * bin_size + 1) + \"-\" + str(i * bin_size + bin_size) for i in\n","                      range(0, int(matrix.shape[1]))]\n","    matrix = matrix.T\n","    x_axis_labels = matrix_.columns  # labels for x-axis\n","    y_axis_labels = matrix.columns  # labels for y-axis\n","    plt.figure(figsize=(0.6 * matrix.shape[1], 0.6 * matrix.shape[0]))\n","    ax = plt.axes()\n","    ax.set_title('Instance Selection Frequencies', fontsize=14, fontweight='bold')\n","    ax.xticklabels = x_axis_labels\n","    ax.yticklabels = y_axis_labels\n","    sn.heatmap(matrix, annot=False, cmap=\"Reds\", fmt='g', ax=ax, square=False)\n","    ax.invert_yaxis()\n","    plt.xticks(rotation=90)\n","    plt.yticks(rotation=0)\n","\n","    # Make all positive labels green, and negative labels red\n","    for lab in ax.get_xticklabels():\n","        text = lab.get_text()\n","        if text in str(target_ids):\n","            lab.set_color('green')\n","        else:\n","            lab.set_color('red')\n","    plt.savefig(\"../Figures/\" + name + \"_heat.png\", bbox_inches='tight')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def color_invalid_red(val):\n","    color = 'red' if val > 0.05 else 'black'\n","    return 'color: %s' % color"]},{"cell_type":"markdown","metadata":{},"source":["calculate the ALC value given a set of AUC scores over the course of training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calc_alc(auc_row):\n","    x = range(len(auc_row))\n","    x = x[1:]\n","    auc_row = auc_row[1:]\n","    x = np.log2(x)\n","    auc_row = auc_row.set_axis(x)\n","    y = auc_row\n","    A = np.trapz(y)\n","    Amax = np.trapz(pd.Series(1, index=x))\n","    Arand = np.trapz(pd.Series(0.5, index=x))\n","    #Arand = rand_predict * x[-1]\n","\n","    #Amax = x[-1]\n","    global_score = (A - Arand) / (Amax - Arand)\n","\n","    #     print(\"ALC is: \", global_score)\n","    return (global_score)"]},{"cell_type":"markdown","metadata":{},"source":["Perform the Wilcoxon signed-rank test for the ALC scores of different methods. Make sure the number of runs and queries per run are the same"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def wsrt(auc_list, method_names):\n","    results_table = np.empty((0, len(method_names)), dtype='float')\n","    for i, auc_table1 in enumerate(auc_list):\n","        results_array = np.empty(len(auc_list))\n","        alc_scores_1 = auc_table1.apply(calc_alc, axis=1)\n","        for j, auc_table2 in enumerate(auc_list):\n","            if i != j:\n","                alc_scores_2 = auc_table2.apply(calc_alc, axis=1)\n","                if len(alc_scores_2) == 25 and len(alc_scores_1) > 25:\n","                    alc_scores_1 = alc_scores_1.drop(alc_scores_1.index[25:])\n","                if len(alc_scores_1) == 25 and len(alc_scores_2) > 25:\n","                    alc_scores_2 = alc_scores_2.drop(alc_scores_2.index[25:])\n","                #alc_scores_1 = np.average(alc_scores_1.to_numpy())\n","                #alc_scores_2 = np.average(alc_scores_2.to_numpy())\n","                p_value = wilcoxon(alc_scores_1, alc_scores_2, correction=False)\n","                results_array[j] = p_value[1]\n","            #                 print(\"For \", result_string1, \" and \", result_string2, \" the p-value given the ALC scores is :\\n\", p_value[1], \"\\n\")\n","            else:\n","                results_array[j] = float('NaN')\n","        #             print(results_array[i])\n","\n","        #         print(results_array)\n","        #         np.append(arr, np.array([[1,2,3]]), axis=0)\n","        #         results_table = np.append(results_table, results_array, axis=0)\n","        results_table = np.vstack((results_table, results_array))\n","    results_table = pd.DataFrame(results_table, index=method_names, columns=method_names)\n","    #     pd.options.display.float_format = '{:.2e}'.format\n","    #     pd.options.display.float_format = '{:.2f}'.format\n","    #     pd.set_option('display.float_format', lambda x: '%.3f' % x)\n","    #     pd.set_option('display.float_format', lambda x: f'{x:,.3f}')\n","    with pd.option_context('display.max_rows', 5, 'display.max_columns', 5):\n","        pd.set_option('display.float_format', '{:.2f}'.format)\n","        #         display(results_table)\n","        #pd.set_option(\"display.precision\", 2)\n","        results_table.style\n","        display(results_table.style.applymap(color_invalid_red).format('{:.2F}', na_rep='NA'))\n","        config = imgkit.config(\n","            wkhtmltoimage='C:\\\\Users\\\\alecf\\\\Documents\\\\AI Master\\\\AL_OpenML_Experiments\\\\wkhtmltoimage.exe')  # , xvfb='/opt/bin/xvfb-run'\n","        html = results_table.style.set_properties(**{'background-color': 'ghostwhite',\n","                                                     'color': 'black',\n","                                                     'border-color': 'white'}).render()\n","        imgkit.from_string(html, '../Wilcoxon_Tables/wsrt_table.png', config=config)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.7 ('deep_learning')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"vscode":{"interpreter":{"hash":"1e5c80a84e3efa5e93a5b1756954e6c5a41e43d247d47c9d90024e451d842940"}}},"nbformat":4,"nbformat_minor":2}
